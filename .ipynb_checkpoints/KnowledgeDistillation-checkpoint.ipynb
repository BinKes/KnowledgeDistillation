{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'name_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-924b241f0586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGaussianNoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mget_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m# learning_phase_scope = tf_keras_backend.learning_phase_scope  # TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mname_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'name_scope'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Input, Embedding, LSTM, Dense, Lambda, GaussianNoise, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Add\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers import MaxPooling2D, Dropout, Dense, Flatten, Activation, Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import natsort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_classes = 10\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plt.imshow(X_train[3])\n",
    "# print(y_train[3])\n",
    "\n",
    "# # convert y_train and y_test to categorical binary values \n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# print(Y_train[3])\n",
    "# X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "# X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "\n",
    "# # Normalize the values\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# def processing_img(filename):\n",
    "#     img = image.load_img('./myntradataset/images/'+filename+'.jpg', target_size=(60,80,3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = img/255\n",
    "#     return img\n",
    "\n",
    "# list_id = list(styles_data['id'].astype('str'))\n",
    "    \n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     for file_id , img in tqdm(zip(list_id ,executor.map(processing_img, list_id )), total=len(list_id)):\n",
    "#         matrix_images.append(img)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# filenames = glob.glob(\"./myntradataset/images/*.jpg\")\n",
    "\n",
    "# filenames = natsort.natsorted(filenames)\n",
    "\n",
    "# data = []\n",
    "# length_data = 1000\n",
    "\n",
    "# for i in range(length_data):\n",
    "#     image = cv2.imread(filenames[i])\n",
    "#     widht = 60\n",
    "#     height = 80\n",
    "#     dim = (widht,height)\n",
    "#     image = cv2.resize(image, dim) \n",
    "#     data.append(image)\n",
    "\n",
    "# data = np.array(data) \n",
    "# data = data.astype('float32')\n",
    "# data = data / 255\n",
    "# # print(data.shape)\n",
    "\n",
    "# styles_data = pd.read_csv('myntradataset/styles.csv', error_bad_lines=False)\n",
    "\n",
    "# styles_data.sort_values(by=['id'], inplace=True)\n",
    "# styles_data = styles_data.iloc[:,2:3]\n",
    "# styles_data = styles_data.to_numpy()\n",
    "# styles_data = styles_data.flatten()\n",
    "\n",
    "# labels = []\n",
    "# mydict = collections.defaultdict(lambda : -1) \n",
    "# inc_tag = 0\n",
    "\n",
    "# for i in range(length_data):\n",
    "#     if mydict[styles_data[i]] == -1:\n",
    "#         mydict[styles_data[i]] = inc_tag\n",
    "#         inc_tag += 1\n",
    "#         labels.append(mydict[styles_data[i]])\n",
    "#     else:\n",
    "#         labels.append(mydict[styles_data[i]])\n",
    "\n",
    "# # print(mydict)\n",
    "# nb_classes = 4\n",
    "# labels = np_utils.to_categorical(labels, nb_classes)        \n",
    "# # print(labels.shape)\n",
    "\n",
    "# input_shape = (80,60,3)\n",
    "\n",
    "# X_train, X_test,y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "# # print(X_test.shape)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# testArray = np.random.rand(100, 9)\n",
    "\n",
    "# for n in range(9):\n",
    "#     print(n)\n",
    "#     testArray = np.delete(testArray,[n],axis=1)\n",
    "#     print(testArray.shape)\n",
    "#     print(testArray[0:1][:])\n",
    "\n",
    "# testArray = np.delete(testArray,[1,2,3])\n",
    "# print(testArray.shape)\n",
    "# print(testArray[0:1][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7613c99674ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnb_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Y.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfile_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "y_data = None\n",
    "x_data = None\n",
    "nb_classes = None\n",
    "\n",
    "if os.path.isfile(\"X.pickle\") and os.path.isfile(\"Y.pickle\"):\n",
    "    \n",
    "    file_pickle = open(\"X.pickle\", \"rb\")\n",
    "    x_data = pickle.load(file_pickle)\n",
    "    file_pickle.close()\n",
    "    \n",
    "    file_pickle = open(\"Y.pickle\", \"rb\")\n",
    "    y_data = pickle.load(file_pickle)\n",
    "    file_pickle.close()\n",
    "    \n",
    "    nb_classes = y_data.shape[1]\n",
    "\n",
    "else:\n",
    "    \n",
    "    styles_data = pd.read_csv('myntradataset/styles.csv', error_bad_lines=False)\n",
    "    matrix_images = []\n",
    "\n",
    "#     styles_data = styles_data.iloc[:600]\n",
    "    styles_data.articleType, uniques_class = pd.factorize(styles_data.articleType)\n",
    "    uniques_class = np.array(uniques_class)\n",
    "    nb_classes = len(uniques_class)\n",
    "    y_data = styles_data['articleType'].values\n",
    "    y_data = to_categorical(y_data)\n",
    "    \n",
    "    missing_file = [\"39403\",\n",
    "                    \"39410\",\n",
    "                    \"39401\",\n",
    "                    \"39425\"]\n",
    "    \n",
    "    drop_index = []\n",
    "\n",
    "    for i in tqdm(range(styles_data.shape[0])):\n",
    "#     for i in tqdm(range(600)):\n",
    "        filename = styles_data['id'][i].astype('str')\n",
    "        if filename not in missing_file:\n",
    "            img = image.load_img('./myntradataset/images/'+styles_data['id'][i].astype('str')+'.jpg', target_size=(60,80,3))\n",
    "            img = image.img_to_array(img)\n",
    "            img = img/255\n",
    "            matrix_images.append(img)\n",
    "        else:\n",
    "            drop_index.append(i)\n",
    "            \n",
    "    x_data = np.array(matrix_images)\n",
    "    y_data = np.delete(y_data, drop_index, axis=0)\n",
    "\n",
    "    file_pickle = open(\"X.pickle\",\"wb\")\n",
    "    pickle.dump(x_data, file_pickle)\n",
    "    file_pickle.close()\n",
    "    \n",
    "    file_pickle = open(\"Y.pickle\",\"wb\")\n",
    "    pickle.dump(y_data, file_pickle)\n",
    "    file_pickle.close()\n",
    "\n",
    "print(x_data.shape, y_data.shape, nb_classes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=42)\n",
    "input_shape = (60,80,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  build teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 58, 78, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 56, 76, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 28, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 68096)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8716416   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 143)               18447     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 143)               0         \n",
      "=================================================================\n",
      "Total params: 8,754,255\n",
      "Trainable params: 8,754,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_teacher_model():\n",
    "#     filters = 64 or 128 \n",
    "    pool_size = (2,2)\n",
    "    kernel_size = (3,3)\n",
    "    \n",
    "    teacher_m = Sequential()\n",
    "    teacher_m.add(Conv2D(\n",
    "        32,\n",
    "        kernel_size = kernel_size,\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    \n",
    "    teacher_m.add(Conv2D(64,kernel_size=kernel_size, activation ='relu'))\n",
    "    \n",
    "    teacher_m.add(MaxPooling2D(pool_size = pool_size))\n",
    "    teacher_m.add(Dropout(0.25))\n",
    "    teacher_m.add(Flatten())\n",
    "    teacher_m.add(Dense(128, activation = 'relu'))\n",
    "    teacher_m.add(Dropout(0.5))\n",
    "    teacher_m.add(Dense(nb_classes))\n",
    "    teacher_m.add(Activation('softmax'))\n",
    "    \n",
    "    teacher_m.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return teacher_m\n",
    "\n",
    "teacher_m = build_teacher_model()\n",
    "print(teacher_m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                460832    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 143)               4719      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 143)               0         \n",
      "=================================================================\n",
      "Total params: 465,551\n",
      "Trainable params: 465,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_student_model():\n",
    "    \n",
    "    student = Sequential()\n",
    "    student.add(Flatten(input_shape=input_shape))\n",
    "    student.add(Dense(32, activation='relu'))\n",
    "    student.add(Dropout(0.2))\n",
    "    student.add(Dense(nb_classes))\n",
    "    student.add(Activation('softmax'))\n",
    "\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    student.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "    return student\n",
    "\n",
    "student = build_student_model()\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29761 samples, validate on 14659 samples\n",
      "Epoch 1/10\n",
      "29761/29761 [==============================] - 260s 9ms/step - loss: 2.6967 - accuracy: 0.3846 - val_loss: 1.3548 - val_accuracy: 0.6585\n",
      "Epoch 2/10\n",
      "29761/29761 [==============================] - 263s 9ms/step - loss: 1.5117 - accuracy: 0.6144 - val_loss: 1.0441 - val_accuracy: 0.7229\n",
      "Epoch 3/10\n",
      "29761/29761 [==============================] - 263s 9ms/step - loss: 1.2250 - accuracy: 0.6762 - val_loss: 0.9283 - val_accuracy: 0.7455\n",
      "Epoch 4/10\n",
      "29761/29761 [==============================] - 263s 9ms/step - loss: 1.0574 - accuracy: 0.7090 - val_loss: 0.8752 - val_accuracy: 0.7601\n",
      "Epoch 5/10\n",
      "29761/29761 [==============================] - 264s 9ms/step - loss: 0.9513 - accuracy: 0.7351 - val_loss: 0.7937 - val_accuracy: 0.7752\n",
      "Epoch 6/10\n",
      "29761/29761 [==============================] - 264s 9ms/step - loss: 0.8522 - accuracy: 0.7619 - val_loss: 0.7761 - val_accuracy: 0.7857\n",
      "Epoch 7/10\n",
      "29761/29761 [==============================] - 262s 9ms/step - loss: 0.7777 - accuracy: 0.7771 - val_loss: 0.7328 - val_accuracy: 0.8003\n",
      "Epoch 8/10\n",
      "29761/29761 [==============================] - 262s 9ms/step - loss: 0.7006 - accuracy: 0.7964 - val_loss: 0.7436 - val_accuracy: 0.8021\n",
      "Epoch 9/10\n",
      "29761/29761 [==============================] - 263s 9ms/step - loss: 0.6500 - accuracy: 0.8081 - val_loss: 0.7128 - val_accuracy: 0.8039\n",
      "Epoch 10/10\n",
      "29761/29761 [==============================] - 263s 9ms/step - loss: 0.5869 - accuracy: 0.8260 - val_loss: 0.6965 - val_accuracy: 0.8112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29c822cc710>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the teacher model as usual\n",
    "epochs = 10\n",
    "batch_size = 56\n",
    "teacher_m.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
